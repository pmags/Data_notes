{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599823546694",
   "display_name": "Python 3.7.6 64-bit ('dataCampStudy': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Merging DataFrames with pandas (DataCamp)\n",
    "---\n",
    "\n",
    "## Ch1: Preparing Data\n",
    "---\n",
    "\n",
    "When data is spread among several files, you usually invoke pandas' read_csv() (or a similar data import function) multiple times to load the data into several DataFrames. (hint use the pandas cheat sheet https://datacamp-community-prod.s3.amazonaws.com/9f0f2ae1-8bd8-4302-a67b-e17f3059d9e8).\n",
    "\n",
    "Loading data from multiple files into DataFrames is more efficient in a *loop* or a *list comprehension*. The first method consists of creating an empty list where each dataframe will the placede:\n",
    "\n",
    "```py\n",
    "# list of files\n",
    "list = [\"file1.csv\", \"file2.csv\"]\n",
    "\n",
    "# create empty list\n",
    "dataframe = []\n",
    "\n",
    "for i in list:\n",
    "    dataframes.append(pd.read_csv(i))\n",
    "```\n",
    "\n",
    "The alternative is to use a comprehension which reproduce the above but with a simpler sintax.\n",
    "\n",
    "```\n",
    "list = [\"file1.csv\", \"file2.csv\"]\n",
    "dataframe = [pd.read_csv(i) for i in list]\n",
    "```\n",
    "\n",
    "Notice that this approach is not restricted to working with CSV files. That is, even if your data comes in other formats, as long as pandas has a suitable data import function, you can apply a loop or comprehension to generate a list of DataFrames imported from the source files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "NOC         Country   Total\n0  USA   United States  2088.0\n1  URS    Soviet Union   838.0\n2  GBR  United Kingdom   498.0\n3  FRA          France   378.0\n4  GER         Germany   407.0\n"
    }
   ],
   "source": [
    "#import pandas library\n",
    "import pandas as pd \n",
    "\n",
    "# importing dataframes\n",
    "bronze = pd.read_csv(\"C:/repos/data_notes/databases/Summer_Olympic_medals/Bronze.csv\")\n",
    "silver = pd.read_csv(\"C:/repos/data_notes/databases/Summer_Olympic_medals/Silver.csv\")\n",
    "gold = pd.read_csv(\"C:/repos/data_notes/databases/Summer_Olympic_medals/Gold.csv\")\n",
    "\n",
    "print(gold.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "NOC         Country   Total\n0  USA   United States  1052.0\n1  URS    Soviet Union   584.0\n2  GBR  United Kingdom   505.0\n3  FRA          France   475.0\n4  GER         Germany   454.0\n"
    }
   ],
   "source": [
    "# using a loop\n",
    "\n",
    "# Create the list of file names: filenames\n",
    "filenames = [\"C:/repos/data_notes/databases/Summer_Olympic_medals/Bronze.csv\",\"C:/repos/data_notes/databases/Summer_Olympic_medals/Silver.csv\", \"C:/repos/data_notes/databases/Summer_Olympic_medals/Gold.csv\"]\n",
    "\n",
    "# Create the list of three DataFrames: dataframes\n",
    "dataframes = []\n",
    "for filename in filenames:\n",
    "    dataframes.append(pd.read_csv(filename))\n",
    "\n",
    "# Print top 5 rows of 1st DataFrame in dataframes\n",
    "print(dataframes[0].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll combine the three DataFrames from earlier exercises - gold, silver, & bronze - into a single DataFrame called medals. The approach you'll use here is clumsy. Later on in the course, you'll see various powerful methods that are frequently used in practice for concatenating or merging DataFrames.\n",
    "\n",
    "Remember, the column labels of each DataFrame are NOC, Country, and Total, where NOC is a three-letter code for the name of the country and Total is the number of medals of that type won."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "NOC         Country    Gold  Silver  Bronze\n0  USA   United States  2088.0  1195.0  1052.0\n1  URS    Soviet Union   838.0   627.0   584.0\n2  GBR  United Kingdom   498.0   591.0   505.0\n3  FRA          France   378.0   461.0   475.0\n4  GER         Germany   407.0   350.0   454.0\n"
    }
   ],
   "source": [
    "# Make a copy of gold: medals\n",
    "medals = gold.copy()\n",
    "\n",
    "# Create list of new column labels: new_labels\n",
    "new_labels = ['NOC', 'Country', 'Gold']\n",
    "\n",
    "# Rename the columns of medals using new_labels\n",
    "medals.columns = new_labels\n",
    "\n",
    "# Add columns 'Silver' & 'Bronze' to medals\n",
    "medals['Silver'] = silver[\"Total\"]\n",
    "medals['Bronze'] = bronze[\"Total\"]\n",
    "\n",
    "# Print the head of medals\n",
    "print(medals.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing dataframes is essencial because they are the means by which dataframes rows are labeled. It is often useful to rearrange the sequence of the rows of a DataFrame by sorting. You don't have to implement these yourself; the principal methods for doing this are .sort_index() and .sort_values().\n",
    "\n",
    "In this exercise, you'll use these methods with a DataFrame of temperature values indexed by month names. You'll sort the rows alphabetically using the Index and numerically using a column. Notice, for this data, the original ordering is probably most useful and intuitive: the purpose here is for you to understand what the sorting methods do.\n",
    "\n",
    "```py\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read 'monthly_max_temp.csv' into a DataFrame: weather1\n",
    "weather1 = pd.read_csv(\"monthly_max_temp.csv\", index_col = \"Month\")\n",
    "\n",
    "# Print the head of weather1\n",
    "print(weather1.head())\n",
    "\n",
    "# Sort the index of weather1 in alphabetical order: weather2\n",
    "weather2 = weather1.sort_index()\n",
    "\n",
    "# Print the head of weather2\n",
    "print(weather2.head())\n",
    "\n",
    "# Sort the index of weather1 in reverse alphabetical order: weather3\n",
    "weather3 = weather1.sort_index(ascending=False)\n",
    "\n",
    "# Print the head of weather3\n",
    "print(weather3.head())\n",
    "\n",
    "# Sort weather1 numerically using the values of 'Max TemperatureF': weather4\n",
    "weather4 = weather1.sort_values('Max TemperatureF', ascending=True)\n",
    "\n",
    "# Print the head of weather4\n",
    "print(weather4.head())\n",
    "```\n",
    "\n",
    "Sorting methods are not the only way to change DataFrame Indexes. There is also the .reindex() method. In this exercise, you'll reindex a DataFrame of quarterly-sampled mean temperature values to contain monthly samples.\n",
    "\n",
    "```\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Reindex weather1 using the list year: weather2\n",
    "weather2 = weather1.reindex(year)\n",
    "\n",
    "# Print weather2\n",
    "print(weather2)\n",
    "\n",
    "# Reindex weather1 using the list year with forward-fill: weather3\n",
    "weather3 = weather1.reindex(year).ffill()\n",
    "\n",
    "# Print weather3\n",
    "print(weather3)\n",
    "```\n",
    "\n",
    "The .ffill() method fills missing values with the one before.\n",
    "\n",
    "Another common technique is to reindex a DataFrame using the Index of another DataFrame. The DataFrame .reindex() method can accept the Index of a DataFrame or Series as input. You can access the Index of a DataFrame with its .index attribute. The DataFrames names_1981 and names_1881 both have a MultiIndex with levels name and gender giving unique labels to counts in each row.\n",
    "Your job here is to use the DataFrame .reindex() and .dropna() methods to make a DataFrame common_names counting names from 1881 that were still popular in 1981.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1935, 1)\n(1587, 1)\n"
    }
   ],
   "source": [
    "names_1981 = pd.read_csv(\"C:/repos/data_notes/databases/Baby names/names1981.csv\", header=None, names=['name','gender','count'], index_col=(0,1))\n",
    "names_1881 = pd.read_csv(\"C:/repos/data_notes/databases/Baby names/names1881.csv\", header=None, names=['name','gender','count'], index_col=(0,1))\n",
    "\n",
    "# Reindex names_1981 with index of names_1881: common_names\n",
    "common_names = names_1981.reindex(names_1881.index)\n",
    "\n",
    "# Print shape of common_names\n",
    "print(common_names.shape)\n",
    "\n",
    "# Drop rows with null counts: common_names\n",
    "common_names = common_names.dropna()\n",
    "\n",
    "# Print shape of new common_names\n",
    "print(common_names.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, ordinary arithmetic operators (like +, -, *, and /) broadcast scalar values to conforming DataFrames when combining scalars & DataFrames in arithmetic expressions. Broadcasting also works with pandas Series and NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Min TemperatureC  Mean TemperatureC  Max TemperatureC\n0         -6.111111          -2.222222          0.000000\n1         -8.333333          -6.111111         -3.888889\n2         -8.888889          -4.444444          0.000000\n3         -2.777778          -2.222222         -1.111111\n4         -3.888889          -1.111111          1.111111\n"
    }
   ],
   "source": [
    "weather = pd.read_csv(\"C:/repos/data_notes/databases/pittsburgh2013.csv\")\n",
    "temps_f = weather[['Min TemperatureF', 'Mean TemperatureF', 'Max TemperatureF']]\n",
    "\n",
    "# Convert temps_f to celsius: temps_c\n",
    "temps_c = (temps_f-32)*5/9\n",
    "\n",
    "# Rename 'F' in column names with 'C': temps_c.columns\n",
    "temps_c.columns = temps_c.columns.str.replace(\"F\", \"C\")\n",
    "\n",
    "# Print first 5 rows of temps_c\n",
    "print(temps_c.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "VALUE\nDATE               \n2014-07-01  17569.4\n2014-10-01  17692.2\n2015-01-01  17783.6\n2015-04-01  17998.3\n2015-07-01  18141.9\n2015-10-01  18222.8\n2016-01-01  18281.6\n2016-04-01  18436.5\n              VALUE\nDATE               \n2008-12-31  14549.9\n2009-12-31  14566.5\n2010-12-31  15230.2\n2011-12-31  15785.3\n2012-12-31  16297.3\n2013-12-31  16999.9\n2014-12-31  17692.2\n2015-12-31  18222.8\n2016-12-31  18436.5\n              VALUE    growth\nDATE                         \n2008-12-31  14549.9       NaN\n2009-12-31  14566.5  0.114090\n2010-12-31  15230.2  4.556345\n2011-12-31  15785.3  3.644732\n2012-12-31  16297.3  3.243524\n2013-12-31  16999.9  4.311144\n2014-12-31  17692.2  4.072377\n2015-12-31  18222.8  2.999062\n2016-12-31  18436.5  1.172707\n"
    }
   ],
   "source": [
    "# Read 'GDP.csv' into a DataFrame: gdp\n",
    "gdp = pd.read_csv(\"C:/repos/data_notes/databases/GDP/gdp_usa.csv\", parse_dates=True, index_col = \"DATE\")\n",
    "\n",
    "# Slice all the gdp data from 2008 onward: post2008\n",
    "post2008 = gdp[\"2008-01-01\":]\n",
    "\n",
    "# Print the last 8 rows of post2008\n",
    "print(post2008.tail(8))\n",
    "\n",
    "# Resample post2008 by year, keeping last(): yearly\n",
    "yearly = post2008.resample(\"A\").last()\n",
    "\n",
    "# Print yearly\n",
    "print(yearly)\n",
    "\n",
    "# Compute percentage growth of yearly: yearly['growth']\n",
    "yearly['growth'] = yearly.pct_change()*100\n",
    "\n",
    "# Print yearly again\n",
    "print(yearly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Open        Close\nDate                                \n2015-01-02  2058.899902  2058.199951\n2015-01-05  2054.439941  2020.579956\n2015-01-06  2022.150024  2002.609985\n2015-01-07  2005.550049  2025.900024\n2015-01-08  2030.609985  2062.139893\n                   Open        Close\nDate                                \n2015-01-02  1340.364425  1339.908750\n2015-01-05  1348.616555  1326.389506\n2015-01-06  1332.515980  1319.639876\n2015-01-07  1330.562125  1344.063112\n2015-01-08  1343.268811  1364.126161\n"
    }
   ],
   "source": [
    "# In this exercise, stock prices in US Dollars for the S&P 500 in 2015 have been obtained from Yahoo Finance. The files sp500.csv for sp500 and exchange.csv for the exchange rates are both provided to you.\n",
    "#Using the daily exchange rate to Pounds Sterling, your task is to convert both the Open and Close column prices.\n",
    "\n",
    "# Read 'sp500.csv' into a DataFrame: sp500\n",
    "sp500 = pd.read_csv(\"C:/repos/data_notes/databases/sp500.csv\", parse_dates=True, index_col='Date')\n",
    "\n",
    "# Read 'exchange.csv' into a DataFrame: exchange\n",
    "exchange = pd.read_csv(\"C:/repos/data_notes/databases/exchange.csv\" , parse_dates=True, index_col='Date')\n",
    "\n",
    "# Subset 'Open' & 'Close' columns from sp500: dollars\n",
    "dollars = sp500[[\"Open\", \"Close\"]]\n",
    "\n",
    "# Print the head of dollars\n",
    "print(dollars.head())\n",
    "\n",
    "# Convert dollars to pounds: pounds\n",
    "pounds = dollars.multiply(exchange[\"GBP/USD\"], axis = \"rows\")\n",
    "\n",
    "# Print the head of pounds\n",
    "print(pounds.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch2: Concatenating data\n",
    "---\n",
    "\n",
    "`.apend()` stacks rows underneath other dataframe. On the other hand `concat()` does join vertically and horizontally. The append method stacks indexes on top of each other resulting in duplication. This can be solved by addind the argument `.reset_index(drop=True)`.\n",
    "\n",
    "```\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load 'sales-jan-2015.csv' into a DataFrame: jan\n",
    "jan = pd.read_csv(\"sales-jan-2015.csv\", index_col=\"Date\", parse_dates=True)\n",
    "\n",
    "# Load 'sales-feb-2015.csv' into a DataFrame: feb\n",
    "feb = pd.read_csv(\"sales-feb-2015.csv\", index_col=\"Date\", parse_dates=True)\n",
    "\n",
    "# Load 'sales-mar-2015.csv' into a DataFrame: mar\n",
    "mar = pd.read_csv(\"sales-mar-2015.csv\", index_col=\"Date\", parse_dates=True)\n",
    "\n",
    "# Extract the 'Units' column from jan: jan_units\n",
    "jan_units = jan['Units']\n",
    "\n",
    "# Extract the 'Units' column from feb: feb_units\n",
    "feb_units = feb['Units']\n",
    "\n",
    "# Extract the 'Units' column from mar: mar_units\n",
    "mar_units = mar['Units']\n",
    "\n",
    "# Append feb_units and then mar_units to jan_units: quarter1\n",
    "quarter1 = jan_units.append(feb_units).append(mar_units)\n",
    "\n",
    "# Print the first slice from quarter1\n",
    "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
    "\n",
    "# Print the second slice from quarter1\n",
    "print(quarter1.loc[\"feb 26, 2015\":\"mar 7, 2015\"])\n",
    "\n",
    "# Compute & print total sales in quarter1\n",
    "print(quarter1.sum())\n",
    "```\n",
    "\n",
    "```\n",
    "# Initialize empty list: units\n",
    "units = []\n",
    "\n",
    "# Build the list of Series\n",
    "for month in [jan, feb, mar]:\n",
    "    units.append(month[\"Units\"])\n",
    "\n",
    "# Concatenate the list: quarter1\n",
    "quarter1 = pd.concat(units, axis=\"rows\")\n",
    "\n",
    "# Print slices from quarter1\n",
    "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
    "print(quarter1.loc['feb 26, 2015':'mar 7, 2015'])\n",
    "```\n",
    "\n",
    "In this exercise, you'll use the Baby Names Dataset (from data.gov) again. This time, both DataFrames names_1981 and names_1881 are loaded without specifying an Index column (so the default Indexes for both are RangeIndexes).\n",
    "\n",
    "You'll use the DataFrame .append() method to make a DataFrame combined_names. To distinguish rows from the original two DataFrames, you'll add a 'year' column to each with the year (1881 or 1981 in this case). In addition, you'll specify ignore_index=True so that the index values are not used along the concatenation axis. The resulting axis will instead be labeled 0, 1, ..., n-1, which is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(19455, 4)\n(1935, 4)\n(21390, 4)\n         name gender  count  year\n1283   Morgan      M     23  1881\n2096   Morgan      F   1769  1981\n14390  Morgan      M    766  1981\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "names_1981 = pd.read_csv(\"C:/repos/data_notes/databases/Baby names/names1981.csv\", header=None, names=['name','gender','count'])\n",
    "names_1881 = pd.read_csv(\"C:/repos/data_notes/databases/Baby names/names1881.csv\", header=None, names=['name','gender','count'])\n",
    "\n",
    "# Add 'year' column to names_1881 and names_1981\n",
    "names_1881['year'] = 1881\n",
    "names_1981['year'] = 1981\n",
    "\n",
    "# Append names_1981 after names_1881 with ignore_index=True: combined_names\n",
    "combined_names = names_1881.append(names_1981, ignore_index=True)\n",
    "\n",
    "# Print shapes of names_1981, names_1881, and combined_names\n",
    "print(names_1981.shape)\n",
    "print(names_1881.shape)\n",
    "print(combined_names.shape)\n",
    "\n",
    "# Print all rows that contain the name 'Morgan'\n",
    "print(combined_names.loc[combined_names[\"name\"] == \"Morgan\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas library\n",
    "import pandas as pd \n",
    "\n",
    "# importing dataframes\n",
    "bronze = pd.read_csv(\"C:/repos/data_notes/databases/Summer_Olympic_medals/Bronze.csv\")\n",
    "silver = pd.read_csv(\"C:/repos/data_notes/databases/Summer_Olympic_medals/Silver.csv\")\n",
    "gold = pd.read_csv(\"C:/repos/data_notes/databases/Summer_Olympic_medals/Gold.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When stacking a sequence of DataFrames vertically, it is sometimes desirable to construct a MultiIndex to indicate the DataFrame from which each row originated. This can be done by specifying the keys parameter in the call to pd.concat(), which generates a hierarchical index with the labels from keys as the outermost index label. So you don't have to rename the columns of each DataFrame as you load it. Instead, only the Index column needs to be specified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total\n       Country               \nbronze United States   1052.0\n       Soviet Union     584.0\n       United Kingdom   505.0\n       France           475.0\n       Germany          454.0\nsilver United States   1195.0\n       Soviet Union     627.0\n       United Kingdom   591.0\n       France           461.0\n       Italy            394.0\ngold   United States   2088.0\n       Soviet Union     838.0\n       United Kingdom   498.0\n       Italy            460.0\n       Germany          407.0\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#empty list\n",
    "medals = []\n",
    "\n",
    "# medal type\n",
    "medal_types = [\"bronze\", \"silver\", \"gold\"]\n",
    "\n",
    "for medal in medal_types:\n",
    "\n",
    "    file_name = \"C:/repos/data_notes/databases/Summer_Olympic_medals/%s_top5.csv\" % medal\n",
    "    \n",
    "    # Read file_name into a DataFrame: medal_df\n",
    "    medal_df = pd.read_csv(file_name, index_col = \"Country\" )\n",
    "    \n",
    "    # Append medal_df to medals\n",
    "    medals.append(medal_df)\n",
    "    \n",
    "# Concatenate medals: medals\n",
    "medals = pd.concat(medals, keys = [\"bronze\", \"silver\", \"gold\"])\n",
    "\n",
    "# Print medals in entirety\n",
    "print(medals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total    454.0\nName: (bronze, Germany), dtype: float64\n                 Total\nCountry               \nFrance           461.0\nItaly            394.0\nSoviet Union     627.0\nUnited Kingdom   591.0\nUnited States   1195.0\n                       Total\n       Country              \nbronze United Kingdom  505.0\ngold   United Kingdom  498.0\nsilver United Kingdom  591.0\n"
    }
   ],
   "source": [
    "# Sort the entries of medals: medals_sorted\n",
    "medals_sorted = medals.sort_index(level=0)\n",
    "\n",
    "# Print the number of Bronze medals won by Germany\n",
    "print(medals_sorted.loc[('bronze','Germany')])\n",
    "\n",
    "# Print data about silver medals\n",
    "print(medals_sorted.loc['silver'])\n",
    "\n",
    "# Create alias for pd.IndexSlice: idx\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# Print all the data on medals won by the United Kingdom\n",
    "print(medals_sorted.loc[idx[:,\"United Kingdom\"],:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Concatenate dataframes: february\n",
    "february = pd.concat(dataframes, keys=[\"Hardware\", \"Software\", \"Service\"], axis = 1)\n",
    "\n",
    "# Print february.info()\n",
    "print(february.info())\n",
    "\n",
    "# Assign pd.IndexSlice: idx\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# Create the slice: slice_2_8\n",
    "slice_2_8 = february.loc['2015-02-02':'2015-02-08', idx[:, 'Company']]\n",
    "\n",
    "# Print slice_2_8\n",
    "print(slice_2_8)\n",
    "\n",
    "# Make the list of tuples: month_list\n",
    "month_list = [('january', jan), ('february', feb), ('march', mar)]\n",
    "\n",
    "# Create an empty dictionary: month_dict\n",
    "month_dict = {}\n",
    "\n",
    "for month_name, month_data in month_list:\n",
    "\n",
    "    # Group month_data: month_dict[month_name]\n",
    "    month_dict[month_name] = month_data.groupby(\"Company\").sum()\n",
    "\n",
    "# Concatenate data in month_dict: sales\n",
    "sales = pd.concat(month_dict)\n",
    "\n",
    "# Print sales\n",
    "print(sales)\n",
    "\n",
    "# Print all sales by Mediacore\n",
    "idx = pd.IndexSlice\n",
    "print(sales.loc[idx[:, 'Mediacore'], :])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "bronze                               silver                                \\\n       NOC               Country   Total    NOC               Country   Total   \n0      USA         United States  1052.0    USA         United States  1195.0   \n1      URS          Soviet Union   584.0    URS          Soviet Union   627.0   \n2      GBR        United Kingdom   505.0    GBR        United Kingdom   591.0   \n3      FRA                France   475.0    FRA                France   461.0   \n4      GER               Germany   454.0    GER               Germany   350.0   \n..     ...                   ...     ...    ...                   ...     ...   \n133    SEN               Senegal     NaN    SEN               Senegal     1.0   \n134    SUD                 Sudan     NaN    SUD                 Sudan     1.0   \n135    TGA                 Tonga     NaN    TGA                 Tonga     1.0   \n136    BDI               Burundi     NaN    BDI               Burundi     NaN   \n137    UAE  United Arab Emirates     NaN    UAE  United Arab Emirates     NaN   \n\n    gold                                \n     NOC               Country   Total  \n0    USA         United States  2088.0  \n1    URS          Soviet Union   838.0  \n2    GBR        United Kingdom   498.0  \n3    FRA                France   378.0  \n4    GER               Germany   407.0  \n..   ...                   ...     ...  \n133  SEN               Senegal     NaN  \n134  SUD                 Sudan     NaN  \n135  TGA                 Tonga     NaN  \n136  BDI               Burundi     1.0  \n137  UAE  United Arab Emirates     1.0  \n\n[138 rows x 9 columns]\n"
    }
   ],
   "source": [
    "# Create the list of DataFrames: medal_list\n",
    "medal_list = [bronze, silver, gold]\n",
    "\n",
    "# Concatenate medal_list horizontally using an inner join: medals\n",
    "medals = pd.concat(medal_list, keys=['bronze', 'silver', 'gold'], axis=1, join=\"inner\")\n",
    "\n",
    "# Print medals\n",
    "print(medals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll compare the historical 10-year GDP (Gross Domestic Product) growth in the US and in China. The data for the US starts in 1947 and is recorded quarterly; by contrast, the data for China starts in 1961 and is recorded annually.\n",
    "\n",
    "You'll need to use a combination of resampling and an inner join to align the index labels. You'll need an appropriate offset alias for resampling, and the method .resample() must be chained with some kind of aggregation method (.pct_change() and .last() in this case).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Empty DataFrame\nColumns: [GDP, VALUE]\nIndex: []\n"
    }
   ],
   "source": [
    "china = pd.read_csv(\"C:/repos/data_notes/databases/GDP/gdp_china.csv\", index_col = \"Year\")\n",
    "us = pd.read_csv(\"C:/repos/data_notes/databases/GDP/gdp_usa.csv\", index_col = \"DATE\")\n",
    "\n",
    "china.index = pd.to_datetime(china.index)\n",
    "us.index = pd.to_datetime(us.index)\n",
    "\n",
    "# Resample and tidyChina. Resamples data by year, calculate a 10 year percentage change plus drops na\n",
    "china_annual = china.resample(\"A\").last().pct_change(10).dropna()\n",
    "us_annual = us.resample(\"A\").last().pct_change(10).dropna()\n",
    "\n",
    "# join both tables with a inner join\n",
    "gdp = pd.concat([china_annual, us_annual], axis = 1, join = \"inner\")\n",
    "\n",
    "# Resample by decade and print\n",
    "print(gdp.resample(\"10A\").last())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH3: Merging Data\n",
    "---\n",
    "\n",
    "The `merge()` function extends `concat()` with the ability to align rows using multiple columns.\n",
    "\n",
    "```py\n",
    "# Merge revenue with managers on 'city': merge_by_city\n",
    "merge_by_city = pd.merge(revenue, managers, on = \"city\")\n",
    "\n",
    "# Print merge_by_city\n",
    "print(merge_by_city)\n",
    "\n",
    "# Merge revenue with managers on 'branch_id': merge_by_id\n",
    "merge_by_id = pd.merge(revenue, managers, on = \"branch_id\")\n",
    "\n",
    "# Print merge_by_id\n",
    "print(merge_by_id)\n",
    "\n",
    "# Merge revenue & managers on 'city' & 'branch': combined\n",
    "combined = pd.merge(revenue, managers, left_on = \"city\", right_on = \"branch\")\n",
    "\n",
    "# Print combined\n",
    "print(combined)\n",
    "```\n",
    "\n",
    "**What tool to user?**\n",
    "\n",
    "- .append() to stack verically\n",
    "- pd.concat() to stack many vertically or horizontally or simpler inner/outer joins on indexes\n",
    "- .join() inner/outer/left/right join on indexes\n",
    "- pd.merge() many joins on multiple columns\n",
    "\n",
    "Similar to pd.merge_ordered(), the pd.merge_asof() function will also merge values in order using the on column, but for each row in the left DataFrame, only rows from the right DataFrame whose 'on' column values are less than the left value will be kept.\n",
    "\n"
   ]
  },
  {
   "source": [
    "# CH4: Case Study\n",
    "---\n",
    "\n",
    "Your first task here is to prepare a DataFrame editions from a tab-separated values (TSV) file.\n",
    "\n",
    "Initially, editions has 26 rows (one for each Olympic edition, i.e., a year in which the Olympics was held) and 7 columns: 'Edition', 'Bronze', 'Gold', 'Silver', 'Grand Total', 'City', and 'Country'.\n",
    "\n",
    "For the analysis that follows, you won't need the overall medal counts, so you want to keep only the useful columns from editions: 'Edition', 'Grand Total', City, and Country."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Edition  Grand Total         City                     Country\n0      1896          151       Athens                      Greece\n1      1900          512        Paris                      France\n2      1904          470    St. Louis               United States\n3      1908          804       London              United Kingdom\n4      1912          885    Stockholm                      Sweden\n5      1920         1298      Antwerp                     Belgium\n6      1924          884        Paris                      France\n7      1928          710    Amsterdam                 Netherlands\n8      1932          615  Los Angeles               United States\n9      1936          875       Berlin                     Germany\n10     1948          814       London              United Kingdom\n11     1952          889     Helsinki                     Finland\n12     1956          885    Melbourne                   Australia\n13     1960          882         Rome                       Italy\n14     1964         1010        Tokyo                       Japan\n15     1968         1031  Mexico City                      Mexico\n16     1972         1185       Munich  West Germany (now Germany)\n17     1976         1305     Montreal                      Canada\n18     1980         1387       Moscow       U.S.S.R. (now Russia)\n19     1984         1459  Los Angeles               United States\n20     1988         1546        Seoul                 South Korea\n21     1992         1705    Barcelona                       Spain\n22     1996         1859      Atlanta               United States\n23     2000         2015       Sydney                   Australia\n24     2004         1998       Athens                      Greece\n25     2008         2042      Beijing                       China\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create file path: file_path\n",
    "file_path = \"C:/repos/data_notes/databases/Summer_Olympic_medals/Summer Olympic medalists 1896 to 2008 - EDITIONS.tsv\"\n",
    "\n",
    "# Load DataFrame from file_path: editions\n",
    "editions = pd.read_csv(file_path, sep=\"\\t\")\n",
    "\n",
    "# Extract the relevant columns: editions\n",
    "editions = editions[[\"Edition\", \"Grand Total\", \"City\", \"Country\"]]\n",
    "\n",
    "# Print editions DataFrame\n",
    "print(editions)"
   ]
  },
  {
   "source": [
    "our task here is to prepare a DataFrame ioc_codes from a comma-separated values (CSV) file.\n",
    "\n",
    "Initially, ioc_codes has 200 rows (one for each country) and 3 columns: 'Country', 'NOC', & 'ISO code'.\n",
    "\n",
    "For the analysis that follows, you want to keep only the useful columns from ioc_codes: 'Country' and 'NOC' (the column 'NOC' contains three-letter codes representing each country)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Country  NOC\n0      Afghanistan  AFG\n1          Albania  ALB\n2          Algeria  ALG\n3  American Samoa*  ASA\n4          Andorra  AND\n             Country  NOC\n196          Vietnam  VIE\n197  Virgin Islands*  ISV\n198            Yemen  YEM\n199           Zambia  ZAM\n200         Zimbabwe  ZIM\n"
    }
   ],
   "source": [
    "file_path = \"C:/repos/data_notes/databases/Summer_Olympic_medals/Summer Olympic medalists 1896 to 2008 - IOC COUNTRY CODES.csv\"\n",
    "\n",
    "# Load DataFrame from file_path: ioc_codes\n",
    "ioc_codes = pd.read_csv(file_path)\n",
    "\n",
    "# Extract the relevant columns: ioc_codes\n",
    "ioc_codes = ioc_codes[[\"Country\", \"NOC\"]]\n",
    "\n",
    "# Print first and last 5 rows of ioc_codes\n",
    "print(ioc_codes.head())\n",
    "print(ioc_codes.tail())\n"
   ]
  },
  {
   "source": [
    "```\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create empty dictionary: medals_dict\n",
    "medals_dict = {}\n",
    "\n",
    "for year in editions['Edition']:\n",
    "\n",
    "    # Create the file path: file_path\n",
    "    file_path = 'summer_{:d}.csv'.format(year)\n",
    "    \n",
    "    # Load file_path into a DataFrame: medals_dict[year]\n",
    "    medals_dict[year] = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract relevant columns: medals_dict[year]\n",
    "    medals_dict[year] = medals_dict[year][[\"Athlete\", \"NOC\", \"Medal\"]]\n",
    "    \n",
    "    # Assign year to column 'Edition' of medals_dict\n",
    "    medals_dict[year]['Edition'] = year\n",
    "    \n",
    "# Concatenate medals_dict: medals\n",
    "medals = pd.concat(medals_dict, ignore_index = True)\n",
    "\n",
    "# Print first and last 5 rows of medals\n",
    "print(medals.head())\n",
    "print(medals.tail())\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "List of medallists at the Games of the Olympiad per edition, sport, discipline, gender and event  \\\n0                                                NaN                                                 \n1  DISCLAIMER: The IOC Research and Reference Ser...                                                 \n2                                                NaN                                                 \n3                                               City                                                 \n4                                             Athens                                                 \n\n  Unnamed: 1 Unnamed: 2  Unnamed: 3     Unnamed: 4 Unnamed: 5 Unnamed: 6  \\\n0        NaN        NaN         NaN            NaN        NaN        NaN   \n1        NaN        NaN         NaN            NaN        NaN        NaN   \n2        NaN        NaN         NaN            NaN        NaN        NaN   \n3    Edition      Sport  Discipline        Athlete        NOC     Gender   \n4       1896   Aquatics    Swimming  HAJOS, Alfred        HUN        Men   \n\n       Unnamed: 7    Unnamed: 8 Unnamed: 9  \n0             NaN           NaN        NaN  \n1             NaN           NaN        NaN  \n2             NaN           NaN        NaN  \n3           Event  Event_gender      Medal  \n4  100m freestyle             M       Gold  \n      List of medallists at the Games of the Olympiad per edition, sport, discipline, gender and event  \\\n29215                                            Beijing                                                 \n29216                                            Beijing                                                 \n29217                                            Beijing                                                 \n29218                                            Beijing                                                 \n29219                                            Beijing                                                 \n\n      Unnamed: 1 Unnamed: 2       Unnamed: 3            Unnamed: 4 Unnamed: 5  \\\n29215       2008  Wrestling  Wrestling Gre-R        ENGLICH, Mirko        GER   \n29216       2008  Wrestling  Wrestling Gre-R  MIZGAITIS, Mindaugas        LTU   \n29217       2008  Wrestling  Wrestling Gre-R       PATRIKEEV, Yuri        ARM   \n29218       2008  Wrestling  Wrestling Gre-R         LOPEZ, Mijain        CUB   \n29219       2008  Wrestling  Wrestling Gre-R        BAROEV, Khasan        RUS   \n\n      Unnamed: 6  Unnamed: 7 Unnamed: 8 Unnamed: 9  \n29215        Men   84 - 96kg          M     Silver  \n29216        Men  96 - 120kg          M     Bronze  \n29217        Men  96 - 120kg          M     Bronze  \n29218        Men  96 - 120kg          M       Gold  \n29219        Men  96 - 120kg          M     Silver  \n"
    }
   ],
   "source": [
    "file_path = \"C:/repos/data_notes/databases/Summer_Olympic_medals/Summer Olympic medalists 1896 to 2008 - ALL MEDALISTS.tsv\"\n",
    "\n",
    "# Load DataFrame from file_path: ioc_codes\n",
    "medals = pd.read_csv(file_path, sep=\"\\t\")\n",
    "\n",
    "print(medals.head())\n",
    "print(medals.tail())"
   ]
  },
  {
   "source": [
    "```\n",
    "# Construct the pivot_table: medal_counts\n",
    "medal_counts = medals.pivot_table(index = \"Edition\", values = \"Athlete\", columns = \"NOC\", aggfunc = \"count\")\n",
    "\n",
    "# Print the first & last 5 rows of medal_counts\n",
    "print(medal_counts.head())\n",
    "print(medal_counts.tail())\n",
    "\n",
    "\n",
    "# Apply the expanding mean: mean_fractions\n",
    "mean_fractions = fractions.expanding().mean()\n",
    "\n",
    "# Compute the percentage change: fractions_change\n",
    "fractions_change = mean_fractions.pct_change() * 100\n",
    "\n",
    "# Reset the index of fractions_change: fractions_change\n",
    "fractions_change = fractions_change.reset_index()\n",
    "\n",
    "# Print first & last 5 rows of fractions_change\n",
    "print(fractions_change.head())\n",
    "print(fractions_change.tail())\n",
    "``` \n",
    "\n",
    "```\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Left join editions and ioc_codes: hosts\n",
    "hosts = pd.merge(editions, ioc_codes, how = \"left\")\n",
    "\n",
    "# Extract relevant columns and set index: hosts\n",
    "hosts = hosts[[\"Edition\", \"NOC\"]].set_index(\"Edition\")\n",
    "\n",
    "# Fix missing 'NOC' values of hosts\n",
    "print(hosts.loc[hosts.NOC.isnull()])\n",
    "hosts.loc[1972, 'NOC'] = 'FRG'\n",
    "hosts.loc[1980, 'NOC'] = 'URS'\n",
    "hosts.loc[1988, 'NOC'] = 'KOR'\n",
    "\n",
    "# Reset Index of hosts: hosts\n",
    "hosts = hosts.reset_index()\n",
    "\n",
    "# Print hosts\n",
    "print(hosts)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}