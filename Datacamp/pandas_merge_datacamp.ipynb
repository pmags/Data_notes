{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599477017698",
   "display_name": "Python 3.7.6 64-bit ('dataCampStudy': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Merging DataFrames with pandas (DataCamp)\n",
    "---\n",
    "\n",
    "## Ch1: Preparing Data\n",
    "---\n",
    "\n",
    "When data is spread among several files, you usually invoke pandas' read_csv() (or a similar data import function) multiple times to load the data into several DataFrames. (hint use the pandas cheat sheet https://datacamp-community-prod.s3.amazonaws.com/9f0f2ae1-8bd8-4302-a67b-e17f3059d9e8).\n",
    "\n",
    "Loading data from multiple files into DataFrames is more efficient in a *loop* or a *list comprehension*. The first method consists of creating an empty list where each dataframe will the placede:\n",
    "\n",
    "```py\n",
    "# list of files\n",
    "list = [\"file1.csv\", \"file2.csv\"]\n",
    "\n",
    "# create empty list\n",
    "dataframe = []\n",
    "\n",
    "for i in list:\n",
    "    dataframes.append(pd.read_csv(i))\n",
    "```\n",
    "\n",
    "The alternative is to use a comprehension which reproduce the above but with a simpler sintax.\n",
    "\n",
    "```\n",
    "list = [\"file1.csv\", \"file2.csv\"]\n",
    "dataframe = [pd.read_csv(i) for i in list]\n",
    "```\n",
    "\n",
    "Notice that this approach is not restricted to working with CSV files. That is, even if your data comes in other formats, as long as pandas has a suitable data import function, you can apply a loop or comprehension to generate a list of DataFrames imported from the source files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "NOC         Country   Total\n0  USA   United States  2088.0\n1  URS    Soviet Union   838.0\n2  GBR  United Kingdom   498.0\n3  FRA          France   378.0\n4  GER         Germany   407.0\n"
    }
   ],
   "source": [
    "#import pandas library\n",
    "import pandas as pd \n",
    "\n",
    "# importing dataframes\n",
    "bronze = pd.read_csv(\"C:/repos/data_notes/databases/Summer_Olympic_medals/Bronze.csv\")\n",
    "silver = pd.read_csv(\"C:/repos/data_notes/databases/Summer_Olympic_medals/Silver.csv\")\n",
    "gold = pd.read_csv(\"C:/repos/data_notes/databases/Summer_Olympic_medals/Gold.csv\")\n",
    "\n",
    "print(gold.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "NOC         Country   Total\n0  USA   United States  1052.0\n1  URS    Soviet Union   584.0\n2  GBR  United Kingdom   505.0\n3  FRA          France   475.0\n4  GER         Germany   454.0\n"
    }
   ],
   "source": [
    "# using a loop\n",
    "\n",
    "# Create the list of file names: filenames\n",
    "filenames = [\"C:/repos/data_notes/databases/Summer_Olympic_medals/Bronze.csv\",\"C:/repos/data_notes/databases/Summer_Olympic_medals/Silver.csv\", \"C:/repos/data_notes/databases/Summer_Olympic_medals/Gold.csv\"]\n",
    "\n",
    "# Create the list of three DataFrames: dataframes\n",
    "dataframes = []\n",
    "for filename in filenames:\n",
    "    dataframes.append(pd.read_csv(filename))\n",
    "\n",
    "# Print top 5 rows of 1st DataFrame in dataframes\n",
    "print(dataframes[0].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll combine the three DataFrames from earlier exercises - gold, silver, & bronze - into a single DataFrame called medals. The approach you'll use here is clumsy. Later on in the course, you'll see various powerful methods that are frequently used in practice for concatenating or merging DataFrames.\n",
    "\n",
    "Remember, the column labels of each DataFrame are NOC, Country, and Total, where NOC is a three-letter code for the name of the country and Total is the number of medals of that type won."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "NOC         Country    Gold  Silver  Bronze\n0  USA   United States  2088.0  1195.0  1052.0\n1  URS    Soviet Union   838.0   627.0   584.0\n2  GBR  United Kingdom   498.0   591.0   505.0\n3  FRA          France   378.0   461.0   475.0\n4  GER         Germany   407.0   350.0   454.0\n"
    }
   ],
   "source": [
    "# Make a copy of gold: medals\n",
    "medals = gold.copy()\n",
    "\n",
    "# Create list of new column labels: new_labels\n",
    "new_labels = ['NOC', 'Country', 'Gold']\n",
    "\n",
    "# Rename the columns of medals using new_labels\n",
    "medals.columns = new_labels\n",
    "\n",
    "# Add columns 'Silver' & 'Bronze' to medals\n",
    "medals['Silver'] = silver[\"Total\"]\n",
    "medals['Bronze'] = bronze[\"Total\"]\n",
    "\n",
    "# Print the head of medals\n",
    "print(medals.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing dataframes is essencial because they are the means by which dataframes rows are labeled. It is often useful to rearrange the sequence of the rows of a DataFrame by sorting. You don't have to implement these yourself; the principal methods for doing this are .sort_index() and .sort_values().\n",
    "\n",
    "In this exercise, you'll use these methods with a DataFrame of temperature values indexed by month names. You'll sort the rows alphabetically using the Index and numerically using a column. Notice, for this data, the original ordering is probably most useful and intuitive: the purpose here is for you to understand what the sorting methods do.\n",
    "\n",
    "```py\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read 'monthly_max_temp.csv' into a DataFrame: weather1\n",
    "weather1 = pd.read_csv(\"monthly_max_temp.csv\", index_col = \"Month\")\n",
    "\n",
    "# Print the head of weather1\n",
    "print(weather1.head())\n",
    "\n",
    "# Sort the index of weather1 in alphabetical order: weather2\n",
    "weather2 = weather1.sort_index()\n",
    "\n",
    "# Print the head of weather2\n",
    "print(weather2.head())\n",
    "\n",
    "# Sort the index of weather1 in reverse alphabetical order: weather3\n",
    "weather3 = weather1.sort_index(ascending=False)\n",
    "\n",
    "# Print the head of weather3\n",
    "print(weather3.head())\n",
    "\n",
    "# Sort weather1 numerically using the values of 'Max TemperatureF': weather4\n",
    "weather4 = weather1.sort_values('Max TemperatureF', ascending=True)\n",
    "\n",
    "# Print the head of weather4\n",
    "print(weather4.head())\n",
    "```\n",
    "\n",
    "Sorting methods are not the only way to change DataFrame Indexes. There is also the .reindex() method. In this exercise, you'll reindex a DataFrame of quarterly-sampled mean temperature values to contain monthly samples.\n",
    "\n",
    "```\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Reindex weather1 using the list year: weather2\n",
    "weather2 = weather1.reindex(year)\n",
    "\n",
    "# Print weather2\n",
    "print(weather2)\n",
    "\n",
    "# Reindex weather1 using the list year with forward-fill: weather3\n",
    "weather3 = weather1.reindex(year).ffill()\n",
    "\n",
    "# Print weather3\n",
    "print(weather3)\n",
    "```\n",
    "\n",
    "The .ffill() method fills missing values with the one before.\n",
    "\n",
    "Another common technique is to reindex a DataFrame using the Index of another DataFrame. The DataFrame .reindex() method can accept the Index of a DataFrame or Series as input. You can access the Index of a DataFrame with its .index attribute. The DataFrames names_1981 and names_1881 both have a MultiIndex with levels name and gender giving unique labels to counts in each row.\n",
    "Your job here is to use the DataFrame .reindex() and .dropna() methods to make a DataFrame common_names counting names from 1881 that were still popular in 1981.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1935, 1)\n(1587, 1)\n"
    }
   ],
   "source": [
    "names_1981 = pd.read_csv(\"C:/repos/data_notes/databases/Baby names/names1981.csv\", header=None, names=['name','gender','count'], index_col=(0,1))\n",
    "names_1881 = pd.read_csv(\"C:/repos/data_notes/databases/Baby names/names1881.csv\", header=None, names=['name','gender','count'], index_col=(0,1))\n",
    "\n",
    "# Reindex names_1981 with index of names_1881: common_names\n",
    "common_names = names_1981.reindex(names_1881.index)\n",
    "\n",
    "# Print shape of common_names\n",
    "print(common_names.shape)\n",
    "\n",
    "# Drop rows with null counts: common_names\n",
    "common_names = common_names.dropna()\n",
    "\n",
    "# Print shape of new common_names\n",
    "print(common_names.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, ordinary arithmetic operators (like +, -, *, and /) broadcast scalar values to conforming DataFrames when combining scalars & DataFrames in arithmetic expressions. Broadcasting also works with pandas Series and NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Min TemperatureC  Mean TemperatureC  Max TemperatureC\n0         -6.111111          -2.222222          0.000000\n1         -8.333333          -6.111111         -3.888889\n2         -8.888889          -4.444444          0.000000\n3         -2.777778          -2.222222         -1.111111\n4         -3.888889          -1.111111          1.111111\n"
    }
   ],
   "source": [
    "weather = pd.read_csv(\"C:/repos/data_notes/databases/pittsburgh2013.csv\")\n",
    "temps_f = weather[['Min TemperatureF', 'Mean TemperatureF', 'Max TemperatureF']]\n",
    "\n",
    "# Convert temps_f to celsius: temps_c\n",
    "temps_c = (temps_f-32)*5/9\n",
    "\n",
    "# Rename 'F' in column names with 'C': temps_c.columns\n",
    "temps_c.columns = temps_c.columns.str.replace(\"F\", \"C\")\n",
    "\n",
    "# Print first 5 rows of temps_c\n",
    "print(temps_c.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "VALUE\nDATE               \n2014-07-01  17569.4\n2014-10-01  17692.2\n2015-01-01  17783.6\n2015-04-01  17998.3\n2015-07-01  18141.9\n2015-10-01  18222.8\n2016-01-01  18281.6\n2016-04-01  18436.5\n              VALUE\nDATE               \n2008-12-31  14549.9\n2009-12-31  14566.5\n2010-12-31  15230.2\n2011-12-31  15785.3\n2012-12-31  16297.3\n2013-12-31  16999.9\n2014-12-31  17692.2\n2015-12-31  18222.8\n2016-12-31  18436.5\n              VALUE    growth\nDATE                         \n2008-12-31  14549.9       NaN\n2009-12-31  14566.5  0.114090\n2010-12-31  15230.2  4.556345\n2011-12-31  15785.3  3.644732\n2012-12-31  16297.3  3.243524\n2013-12-31  16999.9  4.311144\n2014-12-31  17692.2  4.072377\n2015-12-31  18222.8  2.999062\n2016-12-31  18436.5  1.172707\n"
    }
   ],
   "source": [
    "# Read 'GDP.csv' into a DataFrame: gdp\n",
    "gdp = pd.read_csv(\"C:/repos/data_notes/databases/GDP/gdp_usa.csv\", parse_dates=True, index_col = \"DATE\")\n",
    "\n",
    "# Slice all the gdp data from 2008 onward: post2008\n",
    "post2008 = gdp[\"2008-01-01\":]\n",
    "\n",
    "# Print the last 8 rows of post2008\n",
    "print(post2008.tail(8))\n",
    "\n",
    "# Resample post2008 by year, keeping last(): yearly\n",
    "yearly = post2008.resample(\"A\").last()\n",
    "\n",
    "# Print yearly\n",
    "print(yearly)\n",
    "\n",
    "# Compute percentage growth of yearly: yearly['growth']\n",
    "yearly['growth'] = yearly.pct_change()*100\n",
    "\n",
    "# Print yearly again\n",
    "print(yearly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Open        Close\nDate                                \n2015-01-02  2058.899902  2058.199951\n2015-01-05  2054.439941  2020.579956\n2015-01-06  2022.150024  2002.609985\n2015-01-07  2005.550049  2025.900024\n2015-01-08  2030.609985  2062.139893\n                   Open        Close\nDate                                \n2015-01-02  1340.364425  1339.908750\n2015-01-05  1348.616555  1326.389506\n2015-01-06  1332.515980  1319.639876\n2015-01-07  1330.562125  1344.063112\n2015-01-08  1343.268811  1364.126161\n"
    }
   ],
   "source": [
    "# In this exercise, stock prices in US Dollars for the S&P 500 in 2015 have been obtained from Yahoo Finance. The files sp500.csv for sp500 and exchange.csv for the exchange rates are both provided to you.\n",
    "#Using the daily exchange rate to Pounds Sterling, your task is to convert both the Open and Close column prices.\n",
    "\n",
    "# Read 'sp500.csv' into a DataFrame: sp500\n",
    "sp500 = pd.read_csv(\"C:/repos/data_notes/databases/sp500.csv\", parse_dates=True, index_col='Date')\n",
    "\n",
    "# Read 'exchange.csv' into a DataFrame: exchange\n",
    "exchange = pd.read_csv(\"C:/repos/data_notes/databases/exchange.csv\" , parse_dates=True, index_col='Date')\n",
    "\n",
    "# Subset 'Open' & 'Close' columns from sp500: dollars\n",
    "dollars = sp500[[\"Open\", \"Close\"]]\n",
    "\n",
    "# Print the head of dollars\n",
    "print(dollars.head())\n",
    "\n",
    "# Convert dollars to pounds: pounds\n",
    "pounds = dollars.multiply(exchange[\"GBP/USD\"], axis = \"rows\")\n",
    "\n",
    "# Print the head of pounds\n",
    "print(pounds.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch2: Concatenating data\n",
    "---\n",
    "\n",
    "`.apend()` stacks rows underneath other dataframe. On the other hand `concat()` does join vertically and horizontally. The append method stacks indexes on top of each other resulting in duplication. This can be solved by addind the argument `.reset_index(drop=True)`.\n",
    "\n",
    "```\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load 'sales-jan-2015.csv' into a DataFrame: jan\n",
    "jan = pd.read_csv(\"sales-jan-2015.csv\", index_col=\"Date\", parse_dates=True)\n",
    "\n",
    "# Load 'sales-feb-2015.csv' into a DataFrame: feb\n",
    "feb = pd.read_csv(\"sales-feb-2015.csv\", index_col=\"Date\", parse_dates=True)\n",
    "\n",
    "# Load 'sales-mar-2015.csv' into a DataFrame: mar\n",
    "mar = pd.read_csv(\"sales-mar-2015.csv\", index_col=\"Date\", parse_dates=True)\n",
    "\n",
    "# Extract the 'Units' column from jan: jan_units\n",
    "jan_units = jan['Units']\n",
    "\n",
    "# Extract the 'Units' column from feb: feb_units\n",
    "feb_units = feb['Units']\n",
    "\n",
    "# Extract the 'Units' column from mar: mar_units\n",
    "mar_units = mar['Units']\n",
    "\n",
    "# Append feb_units and then mar_units to jan_units: quarter1\n",
    "quarter1 = jan_units.append(feb_units).append(mar_units)\n",
    "\n",
    "# Print the first slice from quarter1\n",
    "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
    "\n",
    "# Print the second slice from quarter1\n",
    "print(quarter1.loc[\"feb 26, 2015\":\"mar 7, 2015\"])\n",
    "\n",
    "# Compute & print total sales in quarter1\n",
    "print(quarter1.sum())\n",
    "```\n",
    "\n",
    "```\n",
    "# Initialize empty list: units\n",
    "units = []\n",
    "\n",
    "# Build the list of Series\n",
    "for month in [jan, feb, mar]:\n",
    "    units.append(month[\"Units\"])\n",
    "\n",
    "# Concatenate the list: quarter1\n",
    "quarter1 = pd.concat(units, axis=\"rows\")\n",
    "\n",
    "# Print slices from quarter1\n",
    "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
    "print(quarter1.loc['feb 26, 2015':'mar 7, 2015'])\n",
    "```\n",
    "\n",
    "In this exercise, you'll use the Baby Names Dataset (from data.gov) again. This time, both DataFrames names_1981 and names_1881 are loaded without specifying an Index column (so the default Indexes for both are RangeIndexes).\n",
    "\n",
    "You'll use the DataFrame .append() method to make a DataFrame combined_names. To distinguish rows from the original two DataFrames, you'll add a 'year' column to each with the year (1881 or 1981 in this case). In addition, you'll specify ignore_index=True so that the index values are not used along the concatenation axis. The resulting axis will instead be labeled 0, 1, ..., n-1, which is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(19455, 4)\n(1935, 4)\n(21390, 4)\n         name gender  count  year\n1283   Morgan      M     23  1881\n2096   Morgan      F   1769  1981\n14390  Morgan      M    766  1981\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "names_1981 = pd.read_csv(\"C:/repos/data_notes/databases/Baby names/names1981.csv\", header=None, names=['name','gender','count'])\n",
    "names_1881 = pd.read_csv(\"C:/repos/data_notes/databases/Baby names/names1881.csv\", header=None, names=['name','gender','count'])\n",
    "\n",
    "# Add 'year' column to names_1881 and names_1981\n",
    "names_1881['year'] = 1881\n",
    "names_1981['year'] = 1981\n",
    "\n",
    "# Append names_1981 after names_1881 with ignore_index=True: combined_names\n",
    "combined_names = names_1881.append(names_1981, ignore_index=True)\n",
    "\n",
    "# Print shapes of names_1981, names_1881, and combined_names\n",
    "print(names_1981.shape)\n",
    "print(names_1881.shape)\n",
    "print(combined_names.shape)\n",
    "\n",
    "# Print all rows that contain the name 'Morgan'\n",
    "print(combined_names.loc[combined_names[\"name\"] == \"Morgan\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas library\n",
    "import pandas as pd \n",
    "\n",
    "# importing dataframes\n",
    "bronze = pd.read_csv(\"C:/repos/data_notes/databases/Summer_Olympic_medals/Bronze.csv\")\n",
    "silver = pd.read_csv(\"C:/repos/data_notes/databases/Summer_Olympic_medals/Silver.csv\")\n",
    "gold = pd.read_csv(\"C:/repos/data_notes/databases/Summer_Olympic_medals/Gold.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When stacking a sequence of DataFrames vertically, it is sometimes desirable to construct a MultiIndex to indicate the DataFrame from which each row originated. This can be done by specifying the keys parameter in the call to pd.concat(), which generates a hierarchical index with the labels from keys as the outermost index label. So you don't have to rename the columns of each DataFrame as you load it. Instead, only the Index column needs to be specified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total\n       Country               \nbronze United States   1052.0\n       Soviet Union     584.0\n       United Kingdom   505.0\n       France           475.0\n       Germany          454.0\nsilver United States   1195.0\n       Soviet Union     627.0\n       United Kingdom   591.0\n       France           461.0\n       Italy            394.0\ngold   United States   2088.0\n       Soviet Union     838.0\n       United Kingdom   498.0\n       Italy            460.0\n       Germany          407.0\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#empty list\n",
    "medals = []\n",
    "\n",
    "# medal type\n",
    "medal_types = [\"bronze\", \"silver\", \"gold\"]\n",
    "\n",
    "for medal in medal_types:\n",
    "\n",
    "    file_name = \"C:/repos/data_notes/databases/Summer_Olympic_medals/%s_top5.csv\" % medal\n",
    "    \n",
    "    # Read file_name into a DataFrame: medal_df\n",
    "    medal_df = pd.read_csv(file_name, index_col = \"Country\" )\n",
    "    \n",
    "    # Append medal_df to medals\n",
    "    medals.append(medal_df)\n",
    "    \n",
    "# Concatenate medals: medals\n",
    "medals = pd.concat(medals, keys = [\"bronze\", \"silver\", \"gold\"])\n",
    "\n",
    "# Print medals in entirety\n",
    "print(medals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total    454.0\nName: (bronze, Germany), dtype: float64\n                 Total\nCountry               \nFrance           461.0\nItaly            394.0\nSoviet Union     627.0\nUnited Kingdom   591.0\nUnited States   1195.0\n                       Total\n       Country              \nbronze United Kingdom  505.0\ngold   United Kingdom  498.0\nsilver United Kingdom  591.0\n"
    }
   ],
   "source": [
    "# Sort the entries of medals: medals_sorted\n",
    "medals_sorted = medals.sort_index(level=0)\n",
    "\n",
    "# Print the number of Bronze medals won by Germany\n",
    "print(medals_sorted.loc[('bronze','Germany')])\n",
    "\n",
    "# Print data about silver medals\n",
    "print(medals_sorted.loc['silver'])\n",
    "\n",
    "# Create alias for pd.IndexSlice: idx\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# Print all the data on medals won by the United Kingdom\n",
    "print(medals_sorted.loc[idx[:,\"United Kingdom\"],:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Concatenate dataframes: february\n",
    "february = pd.concat(dataframes, keys=[\"Hardware\", \"Software\", \"Service\"], axis = 1)\n",
    "\n",
    "# Print february.info()\n",
    "print(february.info())\n",
    "\n",
    "# Assign pd.IndexSlice: idx\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# Create the slice: slice_2_8\n",
    "slice_2_8 = february.loc['2015-02-02':'2015-02-08', idx[:, 'Company']]\n",
    "\n",
    "# Print slice_2_8\n",
    "print(slice_2_8)\n",
    "\n",
    "# Make the list of tuples: month_list\n",
    "month_list = [('january', jan), ('february', feb), ('march', mar)]\n",
    "\n",
    "# Create an empty dictionary: month_dict\n",
    "month_dict = {}\n",
    "\n",
    "for month_name, month_data in month_list:\n",
    "\n",
    "    # Group month_data: month_dict[month_name]\n",
    "    month_dict[month_name] = month_data.groupby(\"Company\").sum()\n",
    "\n",
    "# Concatenate data in month_dict: sales\n",
    "sales = pd.concat(month_dict)\n",
    "\n",
    "# Print sales\n",
    "print(sales)\n",
    "\n",
    "# Print all sales by Mediacore\n",
    "idx = pd.IndexSlice\n",
    "print(sales.loc[idx[:, 'Mediacore'], :])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "bronze                               silver                                \\\n       NOC               Country   Total    NOC               Country   Total   \n0      USA         United States  1052.0    USA         United States  1195.0   \n1      URS          Soviet Union   584.0    URS          Soviet Union   627.0   \n2      GBR        United Kingdom   505.0    GBR        United Kingdom   591.0   \n3      FRA                France   475.0    FRA                France   461.0   \n4      GER               Germany   454.0    GER               Germany   350.0   \n..     ...                   ...     ...    ...                   ...     ...   \n133    SEN               Senegal     NaN    SEN               Senegal     1.0   \n134    SUD                 Sudan     NaN    SUD                 Sudan     1.0   \n135    TGA                 Tonga     NaN    TGA                 Tonga     1.0   \n136    BDI               Burundi     NaN    BDI               Burundi     NaN   \n137    UAE  United Arab Emirates     NaN    UAE  United Arab Emirates     NaN   \n\n    gold                                \n     NOC               Country   Total  \n0    USA         United States  2088.0  \n1    URS          Soviet Union   838.0  \n2    GBR        United Kingdom   498.0  \n3    FRA                France   378.0  \n4    GER               Germany   407.0  \n..   ...                   ...     ...  \n133  SEN               Senegal     NaN  \n134  SUD                 Sudan     NaN  \n135  TGA                 Tonga     NaN  \n136  BDI               Burundi     1.0  \n137  UAE  United Arab Emirates     1.0  \n\n[138 rows x 9 columns]\n"
    }
   ],
   "source": [
    "# Create the list of DataFrames: medal_list\n",
    "medal_list = [bronze, silver, gold]\n",
    "\n",
    "# Concatenate medal_list horizontally using an inner join: medals\n",
    "medals = pd.concat(medal_list, keys=['bronze', 'silver', 'gold'], axis=1, join=\"inner\")\n",
    "\n",
    "# Print medals\n",
    "print(medals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll compare the historical 10-year GDP (Gross Domestic Product) growth in the US and in China. The data for the US starts in 1947 and is recorded quarterly; by contrast, the data for China starts in 1961 and is recorded annually.\n",
    "\n",
    "You'll need to use a combination of resampling and an inner join to align the index labels. You'll need an appropriate offset alias for resampling, and the method .resample() must be chained with some kind of aggregation method (.pct_change() and .last() in this case).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Empty DataFrame\nColumns: [GDP, VALUE]\nIndex: []\n"
    }
   ],
   "source": [
    "china = pd.read_csv(\"C:/repos/data_notes/databases/GDP/gdp_china.csv\", index_col = \"Year\")\n",
    "us = pd.read_csv(\"C:/repos/data_notes/databases/GDP/gdp_usa.csv\", index_col = \"DATE\")\n",
    "\n",
    "china.index = pd.to_datetime(china.index)\n",
    "us.index = pd.to_datetime(us.index)\n",
    "\n",
    "# Resample and tidyChina. Resamples data by year, calculate a 10 year percentage change plus drops na\n",
    "china_annual = china.resample(\"A\").last().pct_change(10).dropna()\n",
    "us_annual = us.resample(\"A\").last().pct_change(10).dropna()\n",
    "\n",
    "# join both tables with a inner join\n",
    "gdp = pd.concat([china_annual, us_annual], axis = 1, join = \"inner\")\n",
    "\n",
    "# Resample by decade and print\n",
    "print(gdp.resample(\"10A\").last())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}